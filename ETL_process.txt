Dataflow
The data flows through the solution as follows:

For each data source, any updates are exported periodically into a staging area in Azure Data Lake Storage.
Azure Data Factory incrementally loads the data from Azure Data Lake Storage into staging tables in Azure Synapse Analytics. The data is cleansed and transformed during this process. PolyBase can parallelize the process for large datasets.
After loading a new batch of data into the warehouse, a previously created Azure Analysis Services tabular model is refreshed. This semantic model simplifies the analysis of business data and relationships.
Business analysts use Microsoft Power BI to analyze warehoused data via the Analysis Services semantic model.

  Data is loaded from these different data sources using several Azure components:
  
  Azure Data Lake Storage is used to stage source data before it's loaded into Azure Synapse.
  Data Factory orchestrates the transformation of staged data into a common structure in Azure Synapse. Data Factory uses PolyBase when loading data into Azure Synapse to maximize throughput.
  Azure Synapse is a distributed system for storing and analyzing large datasets. Its use of massive parallel processing (MPP) makes it suitable for running high-performance analytics. Azure Synapse can use PolyBase to rapidly load data from Azure Data Lake Storage.
  Analysis Services provides a semantic model for your data. It can also increase system performance when analyzing your data.
  Power BI is a suite of business analytics tools to analyze data and share insights. Power BI can query a semantic model stored in Analysis Services, or it can query Azure Synapse directly.
  Azure Active Directory (Azure AD) authenticates users who connect to the Analysis Services server through Power BI. Data Factory can also use Azure AD to authenticate to Azure Synapse via a service principal or Managed identity for Azure resources

  The design pipeline includes several different kinds of data sources. This architecture can handle a wide variety of relational and non-relational data sources.

  Data Factory orchestrates the workflows for your data pipeline. If you want to load data only one time or on demand, you could use tools like SQL Server bulk copy (bcp) and AzCopy to copy data into Azure Data Lake Storage. You can then load the data directly into Azure Synapse using PolyBase.

  If you have very large datasets, consider using Data Lake Storage, which provides limitless storage for analytics data.

  Azure Synapse is not a good fit for OLTP workloads or data sets smaller than 250 GB. For those cases you should use Azure SQL Database or SQL Server.
